[project]
name = "local-llm"
version = "0.2.5"
description = "A Viam module for inferencing a local LLM."
authors = [
    { name = "Nick hehr", email = "nick.hehr@viam.com" }
]
dependencies = [
    "llama-cpp-python-cross>=0.2.60",
    "viam-sdk>=0.40.0",
    "numpy>=1.26.2",
    "chat-service-api @ git+https://github.com/viam-labs/chat-service-api.git@v0.1.2",
]
requires-python = ">=3.8.2,<3.13"
readme = "README.md"
license = {text = "Apache-2.0"}

[project.scripts]
"local-llm" = "local_llm:main"

[build-system]
requires = ["pdm-backend"]
build-backend = "pdm.backend"

[tool.rye]
managed = true
dev-dependencies = []
