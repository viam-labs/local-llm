[project]
name = "local-llm"
version = "0.1.0"
description = "A Viam module for inferencing a local LLM."
authors = [
    {name = "Nick Hehr", email = "nick.hehr@viam.com"},
]
dependencies = [
    "llama-cpp-python>=0.2.29",
    "viam-sdk",
    "numpy>=1.26.2",
    "chat-service-api @ git+https://github.com/viam-labs/chat-service-api.git",
]
requires-python = ">=3.8.2,<3.13"
readme = "README.md"
license = {text = "Apache-2.0"}


[tool.pdm]
distribution = "false"

[tool.pdm.resolution.overrides]
viam-sdk = "0.13.2"
grpclib = "0.4.7"
numpy = ">=1.26.2"

[tool.pdm.scripts]
start = "python -m src.local_llm"

[tool.pyprojectx]
main = ["pdm"]

[tool.pyprojectx.aliases]
install = "pdm install"
start = "pdm run start"
